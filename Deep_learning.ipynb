{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hXMrMjFP3IaC",
        "jds0F-B69rrL",
        "u1Gj5WM_JwVG",
        "Gt322FDIJ2Tm",
        "OkWKIxQmBrfX",
        "5KOUpG_gHydR",
        "7wXFj8e_XvmU",
        "wiQA4oMLX5LL",
        "J_jOm1kuKtPF",
        "WXfxK84Me_ul",
        "zLGnWTBIfFL4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lNB13ietE5Q"
      },
      "source": [
        "<p><img alt=\"Deep-learning logo\" height=\"45px\" src=\"https://www.nicepng.com/png/detail/96-967799_start-free-14-day-trial-artificial-intelligence-solutions.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1>Deep Learning</h1>\n",
        "\n",
        "---\n",
        "1. What is Deep Learning?\n",
        "1. Deep learning Process\n",
        "1. Classification of Neural Networks\n",
        "1. Types of Deep Learning Networks\n",
        "1. Examples of deep learning applications\n",
        "1. Limitations of deep learning\n",
        "1. Some Definitions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXMrMjFP3IaC"
      },
      "source": [
        "#1. What is Deep Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuVTopz09LMj"
      },
      "source": [
        "**Deep Learning** là một phần mềm máy tính mô phỏng mạng lưới tế bào thần kinh trong não. Nó là một nhánh con của học máy dựa trên mạng neuron nhân tạo với học biểu diễn _(representation learning)_. Nó được gọi là học sâu vì nó sử dụng mạng neuron sâu _(deep neural networks)_. Việc học này có thể là supervised, semi-supervised or unsupervised. \n",
        "\n",
        "![hierachy](https://s7280.pcdn.co/wp-content/uploads/2020/07/artificaial-intelligence.png)\n",
        "\n",
        "Deep learning algorithms cấu trúc bằng việc kết nối các layers.\n",
        "- Lớp đầu tiên được gọi là Input Layer\n",
        "- Lớp cuối cùng được gọi là Output Layer\n",
        "- Tất cả các lớp ở giữa được gọi là Hidden Layers. Từ \"deep\" có nghĩa là mạng liên kết các neurons có nhiều hơn hai layer.\n",
        "\n",
        "![WhatIsDeepLearning](https://www.guru99.com/images/tensorflow/083018_0542_WhatisDeepl1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jds0F-B69rrL"
      },
      "source": [
        "#2. Deep Learning Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLdmvRJI91P5"
      },
      "source": [
        "A deep neural network làm tăng độ chính xác của nhiều tác vụ, từ nhận diện hình ảnh đến nhận dạng giọng nói. Chúng có thể học tự động mà không cần kiến ​​thức xác định trước và không cần được lập trình viên mã hóa rõ ràng.\n",
        "\n",
        "![DeepLearningProcess](https://www.guru99.com/images/tensorflow/083018_0542_WhatisDeepl2.png)\n",
        "\n",
        "\n",
        "Để nắm bắt ý tưởng của học sâu, hãy tưởng tượng một gia đình, gồm một đứa trẻ sơ sinh và cha mẹ. Khi Trẻ mới biết đi, nó chỉ đồ vật bằng ngón tay chỏ và luôn nói từ 'mèo'. Cha mẹ của nó lo lắng về việc học này có nó, họ liên tục nói với đứa bé 'Đúng, đó là một con mèo' hoặc 'Không, đó không phải là một con mèo'. Đứa bé vẫn tiếp tục chỉ vào các đồ vật và nói nhưng nó đã dần trở lên chính xác hơn khi chỉ ra có phải 'mèo' hay không. Đứa nhỏ, trong sâu thẳm, không biết tại sao nó có thể nói đó là một con mèo hay không. Nó vừa học được cách phân cấp các đặc điểm phức tạp của một con mèo bằng cách xem xét tổng thể con vật và tiếp tục tập trung vào các chi tiết như đuôi hoặc mũi trước đó để đưa ra quyết định.\n",
        "\n",
        "Mạng neuron làm việc tương tự. Mỗi layer đại diện cho một cấp độ hiểu biết. Mạng neural với 4 layers sẽ phân biệt được những đặc trưng phức tạp của vật thể hơn là cái có 2 layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKMzD84zEILy"
      },
      "source": [
        "Việc học gồm có 2 giai đoạn:\n",
        "- Giai đoạn đầu tiên bao gồm việc áp dụng một phép biến đổi phi tuyến để chuyển data thành thành Input và tạo ra một mô hình thống kê _(như  mạng neuron)_ để có Output. \n",
        "- Giai đoạn thứ hai nhằm mục đích cải thiện mô hình bằng một phương pháp toán học được gọi là đạo hàm.\n",
        "\n",
        "The neural network lặp đi lặp lại 2 giai đoạn trên hàng trăm ngàn lần cho đến khi đạt được độ chính xác chấp nhận được. Cái việc lặp 2 giai đoạn đó gọi là iteration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1Gj5WM_JwVG"
      },
      "source": [
        "#3. Classification of Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPf2CVZF5him"
      },
      "source": [
        "##Shallow neural network\n",
        "chỉ có một hidden layer giữa đầu vào và đầu ra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt322FDIJ2Tm"
      },
      "source": [
        "##Deep neural network\n",
        "có nhiều hơn một hidden layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1bmo0M35uwr"
      },
      "source": [
        "![Deep neural network](https://nordiccoder.com/app/uploads/2019/10/Screen-Shot-2019-10-17-at-3.30.07-PM.png)\n",
        "\n",
        "![Deep neutral network](https://i2.wp.com/nttuan8.com/wp-content/uploads/2019/03/nn_full-2.png?resize=578%2C377&ssl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkWKIxQmBrfX"
      },
      "source": [
        "###**Feedforward**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U63qiINgWTkE"
      },
      "source": [
        "Để nhất quán về mặt ký hiệu, gọi input layer là $a^{(0)} (=x)\n",
        "$ kích thước 2 * 1.\n",
        "\n",
        "![Pheptinh](https://i2.wp.com/nttuan8.com/wp-content/uploads/2019/03/CodeCogsEqn-1.gif?resize=517%2C150&ssl=1)\n",
        "\n",
        "Tương tự ta có:\n",
        "\n",
        "$z^{(2)} =(W^{(2)})^{T} ∗ a^{(1)}+b^{(2)}$\n",
        "\n",
        "$a^{(2)} = σ(z^{(2)})$\n",
        "\n",
        "$z^{(3)} =(W^{(3)})^{T} ∗ a^{(2)}+b^{(3)}$\n",
        "\n",
        "$y^{hat} = a^{(3)} = σ(z^{(3)})$\n",
        "\n",
        "![feedforward](https://i1.wp.com/nttuan8.com/wp-content/uploads/2019/03/fw.png?resize=1024%2C98&ssl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KOUpG_gHydR"
      },
      "source": [
        "###**Backpropagation**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvNXXH08Wh4V"
      },
      "source": [
        "Giả sử $J(W,b,X,Y)$ là một hàm mất mát của bài toán, trong đó $W,b$ là tập hợp tất cả các ma trận trọng số giữa các layers và biases của mỗi layer. $X,Y$ là cặp dữ liệu huấn luyện với mỗi cột tương ứng với một điểm dữ liệu. Để có thể áp dụng các **gradient-based methods** (mà **Gradient Descent** là một ví dụ), chúng ta cần tính được:\n",
        "\n",
        " $\\frac{∂J}{W^{(l)}};\\frac{∂J}{∂b^{(l)}},  l=1,2,…,L$\n",
        "\n",
        "Đạo hàm của hàm mất mát theo _chỉ một thành phần_ của ma trận trọng số của lớp cuối cùng:\n",
        "\n",
        "$\\frac{∂J}{∂w^{(L)}_{ij}}=\\frac{∂J}{∂z^{(L)}_{j}}.\\frac{∂z^{(L)}_{j}}{∂w^{(L)}_{ij}} =e^{(L)}_{j}a^{(L−1)}_{i}$\n",
        "\n",
        "Trong đó $e^{(L)}_{j}=\\frac{∂J}{∂z^{(L)}_{j}}$ thường là một đại lượng dễ tính toán và $\\frac{∂z^{(L)}_{j}}{∂w^{(L)}_{ij}}=a^{(L−1)}_{i}$ vì $ z^{(L)}_{j}=w^{(L)T}_{j}a^{(L−1)}+b^{(L)}_{j}$.\n",
        "\n",
        "Tương tự như thế, đạo hàm của hàm mất mát theo **bias** của layer cuối cùng là: \n",
        "\n",
        "$\\frac{∂J}{∂b^{(L)}_{j}}=\\frac{∂J}{∂z^{(L)}_{j}}.\\frac{∂z^{(L)}_{j}}{∂b^{(L)}_{j}}=e^{(L)}_{j}$\n",
        "\n",
        "Với đạo hàm theo hệ số ở các lớp l thấp hơn, chúng ta hay xem hình dưới đây. Ở đây, tại mỗi unit, tôi đã viết riêng đầu vào z và đầu ra a để các bạn tiện theo dõi.\n",
        "\n",
        "![gradient](https://machinelearningcoban.com/assets/14_mlp/backpropagation.png)\n",
        "\n",
        "Dựa vào hính trên, ta có thể tính được:\n",
        "\n",
        "$\\frac{∂J}{∂w^{(L)}_{ij}}=\\frac{∂J}{∂z^{(L)}_{j}}.\\frac{∂z^{(L)}_{j}}{∂w^{(L)}_{ij}} =e^{(L)}_{j}a^{(L−1)}_{i}$\n",
        "\n",
        "$e^{(l)}_{j} = (w^{(l+1)}_{j:} e^{(l+1)}) f′(z^{(l)}_{j})$\n",
        "\n",
        "$w^{(l+1)}_{j}$: được hiểu là hàng thứ $j$ của ma trận $W^{(l+1)}$ (Chú ý dấu hai chấm, khi không có dấu này, tôi mặc định ký hiệu nó cho vector cột).\n",
        "\n",
        "[Details](https://machinelearningcoban.com/2017/02/24/mlp/)\n",
        "\n",
        "![process](https://i2.wp.com/nttuan8.com/wp-content/uploads/2019/03/backpropagation-2.png?resize=1024%2C168&ssl=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH0opAKcP4s6"
      },
      "source": [
        "####**Backpropagation cho Stochastic Gradient Descent**\n",
        "\n",
        "**Đạo hàm theo từng hệ số** $w^{(l)}_{ij},b^{(l)}_{i}$\n",
        "1. Bước feedforward: Với 1 giá trị đầu vào **x**, tính giá trị đầu ra của network, trong quá trình tính toán, lưu lại các activation $a^{(l)}$ tại mỗi layer.\n",
        "\n",
        "1. Với mỗi unit $j$ ở output layer, tính \n",
        "$e^{(L)}_{j}=\\frac{∂J}{∂z^{(L)}_{j}}$\n",
        "\n",
        "1. Từ đó suy ra: \n",
        "$\\frac{∂J}{∂w^{(L)}_{ij}}= a^{(L−1)}_{i} e^{(L)}_{j} \\frac{∂J}{∂b^{(L)}_{j}} = e^{(L)}_{j}$\n",
        "\n",
        "1. Với $l=L−1,L−2,…,1,$ tính:  \n",
        "$e^{(l)}_{j} = (w^{(l+1)}_{j:} e^{(l+1)}) f′(z^{(l)}_{j})$\n",
        "\n",
        "1. Cập nhật đạo hàm cho từng hệ số:\n",
        "$\\frac{∂J}{∂w^{(L)}_{ij}}= a^{(L−1)}_{i} e^{(L)}_{j} \\frac{∂J}{∂b^{(L)}_{j}} = e^{(L)}_{j}$\n",
        "\n",
        "**Đạo hàm theo ma trận** $W^{(l)}, b^{(l)}$\n",
        "Việc tính toán theo từng hệ số như trên chỉ phù hợp cho việc hiểu nguyên lý tính toán, trong khi lập trình, ta cần tìm cách thu gọn chúng về dạng vector và ma trận để tăng tốc độ cho thuật toán. Đặt $e^{(l)}=[e^{(l)}_{1},e^{(l)}_{2},…,e^{(l)}_{l}]^{T} ∈R^{d^{(l)}×1}$ Ta sẽ có quy tắc tính như sau:\n",
        "\n",
        "1. Bước feedforward: Với 1 giá trị đầu vào **x**, tính giá trị đầu ra của network, trong quá trình tính toán, lưu lại các activation $a^{(l)}$ tại mỗi layer.\n",
        "\n",
        "1. Với output layer, tính:\n",
        "$e^{(L)}=\\frac{∂J}{∂z^{(L)}}$\n",
        "\n",
        "1. Từ đó suy ra: \n",
        "$\\frac{∂J}{∂W^{(l)}}=a^{(l−1)}e^{(l)T} \\frac{∂J}{∂b^{(l)}}=e^{(l)}$\n",
        "\n",
        "1. Với $l=L−1,L−2,…,1,$ tính: \n",
        "$e^{(l)}=(W^{(l+1)}e^{(l+1)})⊙f′(z^{(l)})$\n",
        "\n",
        "trong đó $⊙$ là element-wise product hay Hadamard product tức lấy từng thành phần của hai vector nhân với nhau để được vector kết quả.\n",
        "\n",
        "5. Cập nhật đạo hàm cho ma trận trọng số và vector biases:\n",
        "$\\frac{∂J}{∂W^{(l)}}=a^{(l−1)}e^{(l)T} \\frac{∂J}{∂b^{(l)}}=e^{(l)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUV-UaboW50t"
      },
      "source": [
        "####**Python Code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wXFj8e_XvmU"
      },
      "source": [
        "#####**Code 1**\n",
        "Bài toán dự đoán label khi biết tọa độ (x,y) của một điểm.\n",
        "\n",
        "Áp dụng Batch Gradient Descent cho bài toán này (vì lượng dữ liệu là nhỏ)\n",
        "\n",
        "![model](https://machinelearningcoban.com/assets/14_mlp/ex_nn.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGXAs2iNXmtx"
      },
      "source": [
        "**Tạo dữ liệu giả**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "WArQ1Yr8XHKP",
        "outputId": "d7b685fe-cb09-462b-fd66-612620f58432"
      },
      "source": [
        "#To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "N = 100 # number of points per class\n",
        "d0 = 2 # dimensionality\n",
        "C = 3 # number of classes\n",
        "X = np.zeros((d0, N*C)) # data matrix (each row = single example)(2,300)\n",
        "y = np.zeros(N*C, dtype='uint8') # class labels\n",
        "\n",
        "for j in range(C):\n",
        "  ix = range(N*j,N*(j+1))\n",
        "  r = np.linspace(0.0,1,N) # radius\n",
        "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
        "  X[:,ix] = np.c_[r*np.sin(t), r*np.cos(t)].T\n",
        "  y[ix] = j\n",
        "# lets visualize the data:\n",
        "# plt.scatter(X[:N, 0], X[:N, 1], c=y[:N], s=40, cmap=plt.cm.Spectral)\n",
        "\n",
        "plt.plot(X[0, :N], X[1, :N], 'bs', markersize = 7);\n",
        "plt.plot(X[0, N:2*N], X[1, N:2*N], 'ro', markersize = 7);\n",
        "plt.plot(X[0, 2*N:], X[1, 2*N:], 'g^', markersize = 7);\n",
        "# plt.axis('off')\n",
        "plt.xlim([-1.5, 1.5])\n",
        "plt.ylim([-1.5, 1.5])\n",
        "cur_axes = plt.gca()\n",
        "cur_axes.axes.get_xaxis().set_ticks([])\n",
        "cur_axes.axes.get_yaxis().set_ticks([])\n",
        "\n",
        "# plt.savefig('EX.png', bbox_inches='tight', dpi = 600)\n",
        "plt.show()\n",
        "\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAADrCAYAAABXYUzjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfCElEQVR4nO3de3BU5d0H8O+G3JaAgELBG6Uyo9ZL0QGmfSkDtk7Rai28DCqlRUELDF4qjDijiKIgOryMyvs6ltCLODA6dSKXYhGhqEiA6ksk0sgrFoNcKgEjECXJbrKbnPePx7M5e3Kuu2f3OWfP9zOTAcJeDkvyzbO/5/c8T0RRFBARUf4Vyb4AIqKwYgATEUnCACYikoQBTEQkCQOYiEgSBjARkSTFbm7cv39/ZciQITm6FCKiwvThhx9+pSjKAP3nXQXwkCFDUFNT491VERGFQCQSOWL0eZYgiIgkYQATEUnCACYikoQBTEQkCQOYiEgSBjARkSQMYCIiSRjARESSMICJiCRhABMRScIAJiKShAFMRCQJA5iISBIGMBGRJAxgIiJJGMBERJIwgImIJGEAExFJwgAmIpKEAUxEJAkDmIhIEgYwEZEkDGAiIkkYwEREkjCAiYgkYQATEUnCACYikoQBTEQkCQOYiEgSBjARkSQMYCIiSRjARESSMICJiCRhABMRScIAJiKShAFMRCQJA5iISBIGMBGRJAxgIiJJGMBERJIwgImIJGEAh0hjSyPGvjwWR78+mvZ7IpKDARwilTWV2Hl0J363+XdpvyciORjAIZHoSGD5B8vRqXRia/1WLNu9LPX73cd2y748olBiAIfEhgMbkOhIAABiyRha2ltSv5/xxgx0Kp0yL48olBjAIbGkegnOtp9N/bkTXYF7pOkIXvnnKzIuiyjUGMAFxGxirbahFgdPHzS9X0uiBQ+89UBqVExE+cEADhir7gWzibWlu5YinoxbPm48GceS6iWeXisRWWMAB4xZyOon2bQTa/sb99vWeGPJGLbWb83JNRORsWLZF0DOGYXsqItHAeg+yTbjjRmom12HokgR6mbXybxsIjLBEXCAGIWsOrLVT7JxYo3I/xjAAWIWskaTbJxYI/I/BnBAmIXsXRvvwqPvPGo4ycaJNSJ/YwAHhFknQ7IziW2HthlOsnFijcjfOAnnc40tjZhUNQknm0+adjIkO5PYddeu1IRc6MViwPr1QH09MHQoMHEiUF7u/HZO70+UpYiiKI5vPGLECKWmpiaHl0N6i99bjCfeewK3XHoLNkzekPp81f4q3L3x7lRN+IoBV6S6HkJtzx5g3DggmQRaWoCKCqC4GNi6FRg50v52y5cDc+bY31/LKLAVhSFOKZFI5ENFUUZ0+zwD2L8SHQkMenYQTsdOI1ocxbY7tqVGuddUXoN9J/elbltRUoEVN6/A1GFTZV2ufLEYcMEFQFNT97/r2xdoaOga4ZrdLhIR4Wl1fy2jINc+jtMQp4JmFsAhHy75m1nbWei7HmIx4NVXgcWLxa/xb2vj69eLIDSSTALr1tnfzmxAor2/9jrGjRNB3tws7tvcLD7Onk3/XFMTcN11xqFPocUasI+ZtZ1tOrjJsuvh6eufzudl5of6Nn/HDmDNGqCoqGt0ee+9YnRZXy8+Z6SlBTh0SPze6nZmtPdXWQW5kdZW4MILge3bORImABwB+4Z+jwerUW7dl3Xh6nrYs0eUDGbOBFauFEGmH12OGwdcfHFXCUCvogK45BLx+6FDgZISd9egvb8qkyBvbRXXGrfem4PCgSNgSdTuhjX/uQaD+wxO2+Nhw+QNpm1n8WQc4y8bj/337Jdw1RJo3+ZbSSbFCNUsEIuLxUQYAPz850B7u7vr0N5fNXSoCObmZneP1dQELFsGPPaYu/tZYedGIHESThJtd0PVrVXdJttm/W0WPv7yY8P7Dj9/OGpmhuT/4dVXgVmz7EMuEgHKyoxHlpEIsGCBGPUOHSrCd+ZMIJEwf7zycqCtzXoCLR4Hzj8/s7pueTlw5ox9SDrpsBg8GLjlFnedG5RX7ILwEX13w/zR8/Ffu/+LLWV6sRhwxx3A66/b37a8XARTW5vx35eVieCtqBBBZVUCKCsDKiuBf/9blB2sRpN79gDXXy8m3dwoKwNeegmYMsX8Nk46LHr2FGUNs86NQ4eAzZs5MpbMLIBZgpBA393w+PbHoaDrG0idbAtlS5l+sq2jw/l9zcJX+3dOygWPPgpMm+bsOa+6SkwIutXW1n1ST8uo9GJ07VY16LY24KKLjCcsOTL2hZAPseTQdzdowxcIWUuZltFkm1WoAkCvXmKkN3+++L0X7r/f+W3Xr3f3Q0LVq1f3ST3947rpsDASi5lPWHIS0BcYwHlmdzyQKnQb6WhHfE47CyZOFEHd0AA89JCoe2YrGgXefNP57TPphACMJ/W8eFwnjHqaSQoGcJ45OR4IKOCWMjOvveZ+VPbOO101zfJy8da6b18xuoxEMqt1xuNdpQGzBR9aaieEE5FI14h961br63PzuG4Z9TSTFKwB55nV8UBFKMLLE14u3NqvWavUnj3A7NnuA1gdyakTWSNHAsePi+c4dEjUP+fMAb7+2vljqv2+RhNgRvXTiRPF5+307ClG6Zde6mwizOnjZsKop5mkYBeEJLUNtRi9ajRaE61pn+9X3g/H5h5DRWmORj+ymG1+U1UFjB8vapVuRSLAokWixczt8yaTxpNaaufAJZfY7ymhfY7rrrP+Nyxa5L7v16oLoqMjs9cMMN/XgnKGe0H4jNVCi4Kr/ZrtmdDUBPzsZ5kHSVmZ/UhOHRWvXClCUK0Zv/NOerlCWxrYvNnZnhLa53jwQevriESc/7usrr2xETh82P1j6f+NimJfXqGcYwlCErNShFr7Laj9HLyY0TdjNZGlika799vqyxXaft+33nK2p4TW5ZeLgDMaVdt1PLi99nXr3LW+6csfe/YA/fuL/5P2dnF9bE+TggEsSahOKs50Rr+kRIzczJYNz5+f3dtoo3ADrJcYm9VPrWq2dh0Pbrl9PR9+uKv8UV0NjB2bvnBD/XeOG8fSRJ6xBEG5l+mMfkmJCEkjffqIUV0uTJxo3tJmFqZGXRhOOx7ccvt6quWPWAy48UZ3W25STjGAKfesAs3K1KnA3/9uHGpvvCHCIhc1zEzD1Kze7PXbejevp1r+iMWAefOsF7awPS3vWILIscaWRoz/y3h0KB2ourUKg/sMln1J+acGmjqj72Q5cM+ewJgxxrVao81nvK5hWtWIrZiVNbzk5vUsLhav1wUXiMlOq1V7JSVsT8sztqHl2OL3FmPh9oVQoGD8ZePTznUraGa7eL32GnDXXeZvg1VmrVJOjx0KA/U1PnRIvJ7PPde93W7jRuCXv3S2Y1vPnsCpU+F5/fKIm/FIkOhI4Pn3n0/t9bD5s83YfWx34Z9ebNS/OmOGKCn06GF+7pqqpMT8rb6TY4dyPQL1C/1oe9687iP2deucdaBEIsCWLQzfPGMA59CGAxsQS8ZSf27vaMdvN/4W7975Lm57/bbUZuwFxWoXr5UrRbh2Gq8ETHnxRfNSgtNjh8LIqPxh1zHRo4fop96yBRg9OrfXR91wEi6HllQv6bbYov50Pe7ZdE/q9IuCY9fza7UJOiDCYKrFUmyrDgAuse3O6vUqLRWb3Z865Tx8neyPQY6xBpwjtQ21GPXSKMuNd6LFUVx63qXY+KuNhTMSXrwYWLjQvsZrprraOgysTqEIWw3YCS9eL6sDUXv06FoFyA3fTbEGnGdLdy1FW9J6L9tYMoZ9J/fh/jfvx19/9dc8XVmOZXpOWmkp8Pbb9iMxfQeA/ggefvOny/b1Uuv5iUT3Uob6f/z446KGzA3fXeMIOEeuXnG16ZlueiWREmyfvr0wJucyPSft8ceBJ590fnttB4DTFrEwy+T1suo4scJ3It1wM54c0h8p39jSiHOj52LvzL0Y890x+P1Nv0fv0t6m908oCdy98W7TbSoDRb+IwYnSUuCyy9w9jzrhtGCB+JXf7NYyeb0y3cODK+ocYwB7QHukvPbPU9ZOwc6jO/Hw2w+nHUFk5LNTn+GVf76Sj8vNPXURwwsvAD/9qagTWvF6rwTyRqZ7eIS9G8UF1oCzlOhIYPkHy9GpdGJr/VbsOLwj9ecDpw4AAL5p+8b2cZJKEg+89QAmfn9i8PYCNlp08fHHwNy5YjRktfqK/af+lWk9n90ojnEEnCX9Cce/XvdrtCdNdu+yEci9gNWDNGfNEnXc6dOB884DfvKTrv1/jZSWipVXO3aw/9SvMt3Do7MTuOkm76+nADGAs6Q/4fh483E0J1yOGL7l23PgzHo/9RutA2LryNZW87euZWXApEnAqlXu+k8p/4w2JaqoED84Z80SGw717Sv+rKUowPe+J344kyWWILJgdMKx1URaII8bsjob7eBB95M07e3AsGHhWS4cdHabEt17L3Dxxen3icXEx49/DPziF2IvismTWWYywBFwFpyecKwKXInB6iihceOAAwdYHwwDqw6Kt94yP50jkRDBPX068J3vcERsgAGcBasTjo34tsRgxm7jm1OnRC3XjZYWsT1iAA0aJN6FG30MGiT76iRx2ilx9qz4oc2ly2lYgsiC3bFCjS2NmFQ1ydWmO5ncJ2fsNr7p29f9YyqK2Ms3AI36gwYBJ086u63T2xUcN50S8Xi4dqtzgCPgHNL3BxvRL+Jwcp+8sdrIJRoFli/P7LTfgDTqhzZU3XDTKRGPsz9YhwGcI/r+4N3HdhveThu4Tu+TN1bfXLGY6HawOuLGTHOzaD/LM6sSgtGHW6EsRWg7JcrK7G/L+n8aBnCO6PuDZ7wxo1u9WB+4S3cutb1PXpmdjdazZ/blgzVr8loPdFNOyEYoR81qp0RlpXUIl5dzxaMOAzhH9P3BR5qOdFtqrA/pp6qfsr1P3hkdNPngg9mHZ2srsGyZN9foQCiDMZ+iUWDaNLGdqNEeIL17c7c6A9wNzQP6ibPahlqMXjUarYnWtNsVFxWjbnYdLu9/OQDgmsprsO/kPsvH9mXv8KuvikZ8ty1oeuXlwJkzWX9TWo1ui4rsD+Dw2sCBwIkT+X1OX4nFxNl/mzaJP998c+j7gLkbWg7pJ87M+oOTnUlcW3ktahtqMXzlcHx66lPbx/Zl73CmS1T1FMWTyTir0W2+wxcQ1xPqdjV1NFxVJT6mTQt1+FphAGfJaOLMqj843hHHhL9MwN4Tex0t4vBl77BaG9YvQXWrrS10s+IFWwrhUUUZYR9wlowm2+pm12Ht/63F3RvvTtV0I4ikTkc++s1Ry8ccfv5w1Mz0ealn5EhRC168OPPH6NWLs+KFoLoauPFG0V7Y3i7+X3kyhiMM4CyZTbY9+49n0z6vhq/eFQOuQN3sOhRFAvhm5PLLxTdbprVg7T7ARlta8m2r/1VXA2PHpp8BqH49jBvXfcEN/5/TcBIuC2aTbeeUnYNERyLtSHozFSUVWHHzCkwdZnESsF9levyQ6uWXgTvvNN7wRz2zzGYEla/2slwL5MRdLAb07y86Woz06iW6ZtSVb1n8PwcdJ+FywGyyraW9xVH4AkBLogUPvPUAWtozOHlAtkyOH9KaM0d0QVht+GNTSyyE8AUC+u+wO7JIezKG1cZO110nTtIOYe2YAZwFs8m2DsXiBAgDvux0sKKdcDl4UHyTrVwp9vktKXH+OMmk2GHLasMfiy6JUHQU+Fl9vaj5mikp6arxv/aaebi2tooe81mzxDuqEO2axhpwFuw246naX5U2EWdG7XR4+vqnvby83LB6G/mHPwBr1zp/rOZmcXSRWQ1ZXbKs37zl2zri7JMHMBAnMQCN6EQP/A034zVMRhuCWVNUlz8Hphyh7hVitRvaTTeJr5nZs+1Ht1a14wLFGnAO6RdaBLreC1gfU963L/D888A994jbeaVHD2DFCjG63rxZBPKaNYCiQPn2eRp7AuMnA23FQFFnEdqrNqDu61vE/Xs2ArdNAtavAb4eDBdf7hntB+EVN9cpjd0cQDQqtivt7BTbUTqlrx0XALMaMEfAOWJ0WoZa7w3kwZuA/f7AmzZ5X8Pr6ABmzhQfumVtaj6uHAG8f5H6iU6MGj8BKB4JFCnA0dHA4J3Az3+Hge9u8PTSjEJSZmjnnToHMG5c11FUWurJGG6F6FRl1oBzxGyCLpN6r37LSmns9gcGzLev9ILBsrZEEfD8jyC+kiPiY/eQTuDiD4AL/xf44f8ARZ0ovWIzzn/iWvmvYaFR9wqZNs395vxmyspC0x/OAM4Rswm6TFa2+WaPYKv9gSsqxJp/L5You7DhciCmn/eLaD6KxIi9vbMdH538CPdvvt/xYw8cmNnfhU40KmZEEwnvHjMku6axBJEjdhN0ThktdR518ShPHtu1iRPFCicjxcViw5Urr0yfpItGzftEPbBkDBA3CmCj3wN449M3sO6TdZj4fftv8EBMhPmFm5Mx7MyfH4oJOIAjYN9zsq9w3pjtD9y3b9dWg/rtK//4R/HnbPeNMFA7CPj0PHf3UaDg1qpbcbjpcE5KO16NjAO3gY/VBk3q14j67smqVbFPH+Chh7y/Pp/iCNjnzJY6S+uksDumHOg6RVdr+3ZgzBhPJ+mW/hiIZ/AV3Kl04oY1N+A3P/hNqrSzYbI3E3T6UbMXK/UCsUhDOyFn1KIYj4v9IkpLxYRdSYkoWZSWil+1tw3J6BdgG5qvmS119uUewU6cOQNceKFnbWpXzwY+zmLEWVFSgZZEC6LFUWy7Y1vOSjtedkb4vkdY3etB+8NZUczbFwERuPPni5FvgYYvlyIHkJedFL7Qrx/w3nv2pyk7/CasWwEoTwBXfZnZ5bQkROdGLBnD9L9Ox5hVY1DbUOvbsgQQgNGw+u5nwQLxa3m5/ZLleBx47rn8XaOPMIB9zMtOCt9QSxiLFolvTvUMsdJSUSd++23zAC4pAV58Uay4mzRJfKxahbpnY1AWKqmPvTP3IqKffbPx+ZnPsfPoTkxZO8XzjpMTJ8QgMBCLK3LBqn1RFZCTsr3GGrBPNbY04tzouTgy5wgG9xks+3K8FY0Cjz0GzJtnXEu2qiWqu2bNmGH68Et3LUUkEoGb8lqiU0x0Hjh1AADkd5wUEicdEiFafKHFAPYpbe+vVxNEvmM0WQc4m+izYHUiiVPazfUDuVezn1i1L6oqKkKz+EKLX1k+oG+HOv7NcSx6b1Fa72/oGNUSHaqbXZcqRwztNzTjS/DFqdSFQH1X06eP+W20m/OHCAPYB/Qr3eZumYukIiYtvOr99c1y5jwbccGIjEewgd6r2W9GjhQ7nKm1//Jy4z7ykGEAS6Zf6bbj8A6s/SR9S8cjTUdQuafScYAaha1vljPnWbblCK87Toqy/I4L9BJotfZ/+jTw5z+LMF65UgRzgZ+IYYYBLJl+pdutr9/abUP3lkQL5v19HqqPVDsKUH3YGi1nDou62XW4/crbXXdFqLzuODHYT8iVkycDsjLOShblpULDSTjJ9CvdvmwxbmpVjziym503CtsvvvnC8OTmsEwu7W/cb3ooqqq8Rznm/sfcQGyK7/teYHIsHN+BPlXbUIt/nfqXq/sY1YTVkkNtQy2GVQ5DW7It7bZP7XjKcDlzWNTNrsOVA660vE28Ix7c3moKLAawREt3LXV0eKf+7fPnZz5PqwmrJYcpa6fgk68+Sa3wUm/7yVefpN0/jJNLC8cuRJHNl/u7d76bp6shEhjAEn385ceObqd/+xxLxlI14fvevC9VclAXEehvqy4y0ArscuYMLXh3ATphXYC9Yc0NeboaIoEBLNHCsQvRu7R36s9lPcrS/r6ipAI/vOCHhrXaWDIGBQq21G9BPOF+h7FAL2d2qbahFvWn621v948v/oGzbS7OLiPKEgNYIv0EXFtHW9rftyRaUNNQY9lG1d7Rjtak/Ybnw88fnrZfgrJQweZfbw5Fb/DSXUu7dZaYua3qtpxeS6DbyMhzDGBJjA7tNFLaoxSPjH4EwwYOc/0c0eIoIohg/GXjUTOz+zaiQe8Ndrq4ZH/jfsePuaV+S05r49qNeTLdoIchXjgYwJKYbTWpF0vGsOHABkdhbXRfBQq21m/Fpn9tSgurQugNNvoBYhTK6tLkURc521jH77XxgugFJgAMYGmsVmjpywU/GPgDR2FtRt3vVruQw1dHHWVA+wNky2dbcG3ltWkdIUaj+r0n9to+rvoDy+/YC1wYuBBDEjeHdjpdTjv8/OGpUsM1lddg38l9qb9rbG0E0LWQw3dHHbmk/QES74jjo5Mf4b4378OuY7sMDzCtbah1tBpu29RtuP6S63N67UQqjoADQLu7V/uCdpwbPReAqPHuumtX6u/U8LWqL8eSMUxdP7XbAhCr3uB8buSjfS6r533yvSfTfoAAwOaDm1MdIfpRvdOSz+S1kwP1ToCCjQEcME5KB3Zhc6TpiOECELPe4FxM1pmFq/a5zJ63tqEWn576tNtjJpVkWkeIdsWfk+XIAHA6djpUqwRJLgZwwJiVDrTsShZmLVmxZAwvfPBCWihmOllnN2o2Clf9cy3bvSz1e+0k4jM7n0Gy0+KMsW9pR/WrJ6xGz5KetvfpVDpDt0qQ5GEAB4hRacGodKCWLNoXtJvWPaPFUTwy+pG0yb5F1y1Ca7I1LRQznayzGjXrg1YN1z/t/VPac6n/JnUSUX289//9vu3zq9RRvdMShPY++cK2svBiAAeI21OSn3//edO33fqVcGYjXScjbj3tY238dCPWfbIubUSsD/U71t+BnUd34uG3H057Lu3S4cbWxtS16VcMWlH/nW72Bc73KsFMjplnaBcGdkEEiN0pyfqtFJ+pfsbwcaLFUcz50Zy02xuNdFdPWG064p74/YmoKK0wfHztYylQcOeGO/HQqIdSI9jDTYfTgvZ0/DQA4Ju2b+xeAsSSMdSfsV9WXFFSgeb5FodA+szAgc5ay0J7snKBirg5OXbEiBFKTU33FVXkP7UNtRj+h+GmI2BtyxrQvW2toqQCVw24Cnsa9nQLfaMA19I/FiD22413xFOjV/2y62yVFpVCURQkFBH8EUSwY/oOjB482tPnybWITaccAziYIpHIh4qijNB/niWIAqUeza6n1n614WtWWzbbh8LqLbpZC1y8Q5RO2jraPA9fAGjvbE+FLyBG3pNfl9dSNmiQCFOrD65mIwZwgbIrV2iZ1ZbVfSj0m/hoe4713Ex2melX3g/NjzRDWajg9itvtzy5I1ocxapfrjLscPji7Bd4ae9LWV1LppyUE7iajViCIFy94mrTvYn1pQorjS2NGPLfQ9CasN+dzYq2xGF1bapzys4xrR+XF5fjq4e+Mq1X54pdKUGl//YbNMg8mAcOzGzCjuQzK0FwEi5LhfAN42ZZtJXKmkrEk3Fc2PtCNDQ3ZPz2XzupaHRt+hqz1R6+bck2LKle4tuz3tSgVr9WgvD1Qt5hCSJLVm8jw/QWU9t6dvzs8YzDV92IyGzUbVRjVqCgR6SH4e2DtLkO68LhwxEweULfenZR74vQFG9Cc0K0gl0x4AqsnrAaY14e061E0a+8H47NPeaoTLB011LEEt2XUZf2KLXszAiKMP3QJo6AySP6BRvHm4+nwhcQCzju3XSvq4UkRsz2dJB5xJJRxwOREwxgyppRWUBfgsi0rU2vbnZdt9NBKkoqsHrCaseThV7jqJUyxRIEZc1p65kXZQKr/TCsVufJpHY6cGRMehwBU9ac7rPgRZnA7X4YRH7GEXCWrNbwh2XDFK/a2Jxwux+G16zaDu043e+BwoMBnCX2beZXPsPeSDYBqn6t2PWOU3gwgInyjD+0ScUaMBGRJAxgIgv6Hl8iLzGAJbPatpDLUnPP7vXnpBnlEgNYMu4lIVeuX39OqpEVTsL5nNnb3qDstBZmPL2C7HAEHFDq7lksVRAFFwO4AHhdqijEurTZv4lIJgYwdSO7Lp2LHwCsp5MfMYBDzG+jQvV6ZP8A8AIn38gJBrBkMr9RZYaZUfg7vR6/lUYGDhQTbtoPTpCSE+yCkMzqGzWb0ajfe1i9vrZsHy+TLSPZ5UDZ4gjYx7IZHec7fP0ycee3sgqRFQawj5040fWW1iqM/VBv9EvdNl/P5YfXnIKPJYiA8EtNsRCDx82/iWUH8hIDmFzJ5geB3+rSDFOSjSUIyopac3XCT+FL5AcMYALQ1UrlhHaiyy+h6lVpxO+1diosLEEUKLfnj+XqtrmQy9KBX2rtFA4cAReoTIMkFyNAr0eOVm1uHKVSkHAETGm8HgHmY6JLOyLXXj8PvyS/YwBTwWI5gfyOJQjKG6ejziJ+VVJIcARMOaMP3GxGpFxOTIWIY40C5qbO6dVttTuDsQRAZI0j4AKWqwBksBJ5gyNgCgQukKBCxBEwBQJH3VSIOAImIpKEAUxEJAkDmIhIEgYwEZEkDGAiIkkYwEREkjCAiYgkYQATEUnCACYikoQBTEQkCQOYiEgSBjARkSQMYCIiSRjARESSMICJiCRhABMRScIAJiKShAFMRCQJA5iISBIGMBGRJAxgIiJJGMBERJIwgImIJGEAExFJwgAmIpKEAUxEJAkDmIhIEgYwEZEkDGAiIkkYwEREkjCAiYgkYQATEUnCACYikoQBTEQkCQOYiEgSBjARkSQMYCIiSRjARESSRBRFcX7jSKQRwJHcXQ4RUUH6rqIoA/SfdBXARETkHZYgiIgkYQATEUnCACYikoQBTEQkCQOYiEgSBjARkSQMYCIiSRjARESSMICJiCT5f+idyFoHuptcAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(2, 300) (300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iHq-feIdNlQ"
      },
      "source": [
        "**Một số hàm phụ trợ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIogvCv1YboC"
      },
      "source": [
        "def softmax(V):\n",
        "    e_V = np.exp(V - np.max(V, axis = 0, keepdims = True))\n",
        "    Z = e_V / e_V.sum(axis = 0)\n",
        "    return Z\n",
        "\n",
        "## One-hot coding\n",
        "from scipy import sparse\n",
        "def convert_labels(y, C = 3):\n",
        "    Y = sparse.coo_matrix((np.ones_like(y),\n",
        "        (y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
        "    return Y\n",
        "\n",
        "# cost or loss function\n",
        "def cost(Y, Yhat):\n",
        "    return -np.sum(Y*np.log(Yhat))/Y.shape[1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQZxPxxWdSv2"
      },
      "source": [
        "**Tính toán Feedforward**\n",
        "\n",
        "$Z^{(1)}=W^{(1)T}X$\n",
        "\n",
        "$A^{(1)}=max(Z^{(1)},0) $\n",
        "\n",
        "$Z^{(2)}=W^{(2)T}A^{(1)} $\n",
        "\n",
        "$Y^{hat}=A^{(2)} = softmax(Z^{(2)})$\n",
        "\n",
        "**Tính toán Backpropagation**\n",
        "\n",
        "$E^{(2)}=\\frac{∂J}{∂Z^{(2)}}=\\frac{1}{N}(Y^{hat}−Y)$ \n",
        "\n",
        "$\\frac{∂J}{∂W^{(2)}}=A^{(1)}E^{(2)T}$ \n",
        "\n",
        "$\\frac{∂J}{∂b^{(2)}} = ∑_{n=1}^{N} e^{(2)}_{n}$\n",
        "\n",
        "\n",
        "$E^{(1)}=(W^{(2)}E^{(2)})⊙f′(Z^{(1)})$\n",
        "\n",
        "$\\frac{∂J}{∂W^{(1)}}=A^{(0)}E^{(1)T}=XE^{(1)T}$\n",
        "\n",
        "$\\frac{∂J}{∂b^{(1)}}=∑_{n=1}^{N}e^{(1)}_{n}$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeXER6x7ZLIm",
        "outputId": "1804c3d9-1853-4dd6-fc20-0df854b86176"
      },
      "source": [
        "d0 = 2\n",
        "d1 = h = 100 # size of hidden layer\n",
        "d2 = C = 3\n",
        "# initialize parameters randomly\n",
        "W1 = 0.01*np.random.randn(d0, d1)\n",
        "b1 = np.zeros((d1, 1))\n",
        "W2 = 0.01*np.random.randn(d1, d2)\n",
        "b2 = np.zeros((d2, 1))\n",
        "\n",
        "Y = convert_labels(y, C)\n",
        "N = X.shape[1]\n",
        "eta = 1 # learning rate\n",
        "for i in range(10000):\n",
        "    ## Feedforward\n",
        "    Z1 = np.dot(W1.T, X) + b1\n",
        "    A1 = np.maximum(Z1, 0) #activation function ReLU\n",
        "    \n",
        "    Z2 = np.dot(W2.T, A1) + b2\n",
        "    Yhat = softmax(Z2) #activation function sofmax\n",
        "\n",
        "    # print loss after each 1000 iterations\n",
        "    if i %1000 == 0:\n",
        "        # compute the loss: average cross-entropy loss\n",
        "        loss = cost(Y, Yhat)\n",
        "        print(\"iter %d, loss: %f\" %(i, loss))\n",
        "\n",
        "    # backpropagation\n",
        "    E2 = (Yhat - Y )/N\n",
        "    dW2 = np.dot(A1, E2.T)\n",
        "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
        "\n",
        "    E1 = np.dot(W2, E2)\n",
        "    E1[Z1 <= 0] = 0 # gradient of ReLU\n",
        "    dW1 = np.dot(X, E1.T)\n",
        "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
        "\n",
        "    # Gradient Descent update\n",
        "    W1 += -eta*dW1\n",
        "    b1 += -eta*db1\n",
        "    W2 += -eta*dW2\n",
        "    b2 += -eta*db2\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iter 0, loss: 1.098443\n",
            "iter 1000, loss: 0.116711\n",
            "iter 2000, loss: 0.052075\n",
            "iter 3000, loss: 0.037044\n",
            "iter 4000, loss: 0.030016\n",
            "iter 5000, loss: 0.026162\n",
            "iter 6000, loss: 0.023709\n",
            "iter 7000, loss: 0.021936\n",
            "iter 8000, loss: 0.020619\n",
            "iter 9000, loss: 0.019602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23z_DlRGZ6nK",
        "outputId": "263e115c-8d83-4ffa-a60e-1e68d24bf80b"
      },
      "source": [
        "Z1 = np.dot(W1.T, X) + b1\n",
        "A1 = np.maximum(Z1, 0)\n",
        "Z2 = np.dot(W2.T, A1) + b2\n",
        "predicted_class = np.argmax(Z2, axis=0)\n",
        "\n",
        "\n",
        "print('training accuracy: %.2f %%' % (100*np.mean(predicted_class == y)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training accuracy: 99.33 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDA2Q1R0aBnK"
      },
      "source": [
        "Kết quả khi sử dụng 1 hidden layer với 100 units.\n",
        "![image](https://machinelearningcoban.com/assets/14_mlp/ex_res100.png)\n",
        "\n",
        "Kết quả với số lượng units trong hidden layer là khác nhau. \n",
        "![vidu2](https://machinelearningcoban.com/assets/14_mlp/ex_res5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiQA4oMLX5LL"
      },
      "source": [
        "#####**Code 2**\n",
        "\n",
        "Phân loại số viết tay trong tập dữ liệu minist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k5xUYaPhoaL"
      },
      "source": [
        "**Load data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk4YcbIUiCtk",
        "outputId": "5d67fd36-05b8-4545-f4cb-8fec21fdcbb6"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(x_train.shape,y_train.shape)\n",
        "print(x_test.shape,y_test.shape)\n",
        "\n",
        "x = x_train/255.\n",
        "x=x_train.reshape((x_train.shape[0], -1))\n",
        "#to one-hot coding\n",
        "y= to_categorical(y_train)\n",
        "\n",
        "x_test=x_test/255.\n",
        "x_test=x_test.reshape((x_test.shape[0], -1))\n",
        "y_test=to_categorical(y_test)\n",
        "\n",
        "print(x.shape,y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28) (60000,)\n",
            "(10000, 28, 28) (10000,)\n",
            "(60000, 784) (60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCrHFwJSi-cx"
      },
      "source": [
        "# Hàm Relu \n",
        "def Relu(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "#Đạo hàm Relu\n",
        "def Relu_derivative(x):\n",
        "  X_ = np.ones_like(x)\n",
        "  X_[x<=0] = 0\n",
        "  return X_\n",
        "\n",
        "def softmax(Z):\n",
        "  Ev=np.exp(Z-np.max(Z,axis=0,keepdims=True))\n",
        "  return Ev / Ev.sum(axis=0, keepdims=True)\n",
        "\n",
        "def softmax_derivative(Z):\n",
        "  return softmax(Z)*(1-softmax(Z))\n",
        "\n",
        "# Lớp neural network\n",
        "class NeuralNetwork:\n",
        "  def __init__(self, layers, alpha=0.1):\n",
        "    self.layers = layers \n",
        "    \n",
        "    # Hệ số learning rate\n",
        "    self.alpha = alpha\n",
        "  \n",
        "    # Tham số W, b\n",
        "    self.W =[]\n",
        "    self.b =[]\n",
        "    \n",
        "    # Khởi tạo các tham số ở mỗi layer\n",
        "    for i in range(0, len(layers)-1):\n",
        "      w_ = np.random.randn(layers[i]['units'], layers[i+1]['units'])\n",
        "      b_ = np.zeros((layers[i+1]['units'], 1))\n",
        "      self.W.append(0.001*w_)\n",
        "      self.b.append(b_)\n",
        "\n",
        "          \n",
        "  \n",
        "# Tóm tắt mô hình neural network\n",
        "  def __repr__(self):\n",
        "    return \"Neural network [{}]\".format(\"-\".join(str(l) for l in self.layers))\n",
        "  \n",
        "  def feedforward(self, x):\n",
        "    A = [x]\n",
        "    out = A[-1]\n",
        "    for i in range(0, len(self.layers) - 1):\n",
        "      if (self.layers[i]['activation']=='Relu'):\n",
        "        out = Relu(np.dot( out,self.W[i]) + self.b[i].T)\n",
        "      else:\n",
        "        out = softmax(np.dot(out,self.W[i]) + self.b[i].T)\n",
        "      A.append(out)\n",
        "    return A\n",
        "\n",
        "# Train mô hình với dữ liệu\n",
        "  def fit_partial(self, x, y):\n",
        "\n",
        "    # quá trình feedforward\n",
        "    A = self.feedforward(x)\n",
        "\n",
        "    # quá trình backpropagation\n",
        "    # dA = [-(y/A[-1] - (1-y)/(1-A[-1]))]\n",
        "    dA = [(A[-1]-y)*1/x.shape[0]]\n",
        "    dW = []\n",
        "    db = []\n",
        "    for i in reversed(range(0, len(self.layers)-1)):\n",
        "      if(self.layers[i]['activation']=='Relu'):\n",
        "\n",
        "        dw_ = np.dot((A[i]).T, dA[-1] * Relu_derivative(A[i+1]))\n",
        "        db_ = (np.sum(dA[-1] * Relu_derivative(A[i+1]), 0)).reshape(-1,1)\n",
        "        dA_ = np.dot(dA[-1] * Relu_derivative(A[i+1]), self.W[i].T)\n",
        "        \n",
        "      else:\n",
        "        dw_ = np.dot((A[i]).T, dA[-1] * softmax_derivative(A[i+1]))\n",
        "        db_ = (np.sum(dA[-1] * softmax_derivative(A[i+1]), 0)).reshape(-1,1)\n",
        "        dA_ = np.dot(dA[-1] * softmax_derivative(A[i+1]), self.W[i].T)\n",
        "\n",
        "\n",
        "      dW.append(dw_)\n",
        "      db.append(db_)\n",
        "      dA.append(dA_)\n",
        "    \n",
        "    # Đảo ngược dW, db\n",
        "    dW = dW[::-1]\n",
        "    db = db[::-1]\n",
        "      \n",
        "    # Gradient descent\n",
        "    for i in range(0, len(self.layers)-1):\n",
        "      self.W[i] = self.W[i] - self.alpha * dW[i]\n",
        "      self.b[i] = self.b[i] - self.alpha * db[i]\n",
        "    \n",
        "  def fit(self, X, y, epochs=20, verbose=10):\n",
        "    for epoch in range(0, epochs):\n",
        "      self.fit_partial(X, y)\n",
        "      # if epoch % verbose == 0:\n",
        "        # loss = self.calculate_loss(X, y)\n",
        "      #   print(\"Epoch {}, loss {}\".format(epoch, loss))\n",
        "      loss = self.calculate_loss(X, y)\n",
        "      print(\"Epoch {}, loss {}\".format(epoch, loss))\n",
        "  \n",
        "# Dự đoán\n",
        "  def predict(self, X):\n",
        "    return self.feedforward(x)[-1]\n",
        "\n",
        "# Tính loss function\n",
        "  def calculate_loss(self, X, y):\n",
        "    y_predict = self.predict(X)\n",
        "    return self.Loss_func(y,y_predict)\n",
        "\n",
        "  def Loss_func(self,Y,Yhat):\n",
        "    return -np.sum(Y*np.log(Yhat))*1/Y.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBnUXbFHjDVM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "2f03c05d-190d-44e3-daaf-6eeb2090e39b"
      },
      "source": [
        "Layers = [\n",
        "  {'units':784,'activation' : 'Relu'},#input \n",
        "  {'units':512,'activation' : 'Relu'},\n",
        "  {'units':512, 'activation' : 'Relu' },# hidden \n",
        "  {'units':512,'activation' : 'Relu'}, # hidden \n",
        "  {'units':256,'activation' : 'Softmax'},\n",
        "  {'units':10} # output \n",
        "]\n",
        "\n",
        "DNN = NeuralNetwork(Layers)\n",
        "DNN.fit(x,y,80,10)\n",
        "# DNN.feedforward(x,y)\n",
        "# DNN.fit_partial(x,y)\n",
        "print(DNN.calculate_loss(x,y))\n",
        "# DNN.predict(x)[0];"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, loss 11.002099839861822\n",
            "Epoch 1, loss 11.00209983986139\n",
            "Epoch 2, loss 11.002099839860945\n",
            "Epoch 3, loss 11.002099839860508\n",
            "Epoch 4, loss 11.002099839860072\n",
            "Epoch 5, loss 11.002099839859627\n",
            "Epoch 6, loss 11.002099839859198\n",
            "Epoch 7, loss 11.002099839858756\n",
            "Epoch 8, loss 11.002099839858309\n",
            "Epoch 9, loss 11.002099839857882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-66be9f454fb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mDNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mDNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# DNN.feedforward(x,y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# DNN.fit_partial(x,y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-ac15fd43eeac>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, epochs, verbose)\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m       \u001b[0;31m# if epoch % verbose == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# loss = self.calculate_loss(X, y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-ac15fd43eeac>\u001b[0m in \u001b[0;36mfit_partial\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'activation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'Relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mdw_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mRelu_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mdb_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mRelu_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mdA_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mRelu_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-ac15fd43eeac>\u001b[0m in \u001b[0;36mRelu_derivative\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mRelu_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mX_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mX_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mX_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_jOm1kuKtPF"
      },
      "source": [
        "#4. Types of Deep Learning Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz9sQ6pKKxtP"
      },
      "source": [
        "![typeofDNN](https://www.guru99.com/images/tensorflow/083018_0542_WhatisDeepl3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGdywnfbRz3D"
      },
      "source": [
        "##Perception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NamBSbFnMWGe"
      },
      "source": [
        "##Feed-forward Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qtkktox2NK_L"
      },
      "source": [
        "![Feed-forward](https://d1m75rqqgidzqn.cloudfront.net/2019/11/feed-foward-nn-infograph1-300x226.jpg)\n",
        "\n",
        "**Applications:**\n",
        "- Phân loại đơn giản (trong đó các thuật toán phân loại dựa trên Máy học truyền thống có những hạn chế)\n",
        "- Nhận dạng khuôn mặt (Xử lý hình ảnh chuyển tiếp thẳng đơn giản)\n",
        "- Computer vision (Trường hợp các lớp mục tiêu khó phân loại)\n",
        "- Speech Recognition\n",
        "\n",
        "Loại mạng neuron nhân tạo đơn giản nhất. Với kiểu kiến ​​trúc này, thông tin chỉ lưu chuyển theo một hướng, về phía trước. Nó có nghĩa là, các luồng thông tin bắt đầu từ lớp đầu vào, đi đến các lớp \"ẩn\" và kết thúc ở lớp đầu ra. The network không có vòng lặp. Thông tin dừng lại ở các lớp đầu ra.\n",
        "\n",
        "**Advantages:**\n",
        "- Less complex, easy to design and maintain\n",
        "- Fast and speedy\n",
        "- Nhạy cảm với nhiễu\n",
        "\n",
        "**Disadvantages:**\n",
        "- Không thể được sử dụng cho deep learning (do không có lớp dày đặc và back propagation)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10NC6z2_SD-D"
      },
      "source": [
        "##Multilayer Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GstMZL58MV3V"
      },
      "source": [
        "##Recurrent neural networks (RNNs) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtheyulrM3R2"
      },
      "source": [
        "##Convolutional neural networks (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WWK3bdtSzkH"
      },
      "source": [
        "CNN là mạng neuron nhiều lớp với kiến ​​trúc đặc biệt được thiết kế để trích xuất các đặc trưng ngày càng phức tạp của dữ liệu ở mỗi lớp để xác định đầu ra. CNN rất phù hợp cho các bài toán nhận diện.\n",
        "\n",
        "![Convolutional Neural Network](https://www.guru99.com/images/tensorflow/083018_0542_WhatisDeepl5.png)\n",
        "\n",
        "CNN chủ yếu được sử dụng khi cần trích xuất thông tin từ một tập dữ liệu phi cấu trúc (ví dụ: hình ảnh).\n",
        "\n",
        "*Ví dụ:* nếu nhiệm vụ là dự đoán caption của hình ảnh:\n",
        "- Giả sử CNN nhận được một hình ảnh của một con mèo, ảnh này, trong máy tính là một tập các pixel. Thông thường, ảnh sẽ được biểu diễn thành một lớp greyscale(độ xám) hoặc 3 lớp để lưu màu của ảnh.\n",
        "- Trong quá trình feature learning (tức là các lớp ẩn), the network sẽ xác định các đặc điểm đặc biệt, chẳng hạn như đuôi của mèo, tai, v.v.\n",
        "- Khi the network đã học kỹ lưỡng cách nhận dạng một bức ảnh, nó có thể cung cấp một xác suất cho mỗi bức ảnh mà nó biết. Label có xác suất cao nhất sẽ là kết quả dự đoán của the network.\n",
        "\n",
        "**Applications:**\n",
        "- Image processing\n",
        "- Computer Vision\n",
        "- Speech Recognition\n",
        "- Machine translation\n",
        "\n",
        "**Advantages:**\n",
        "- Dùng cho deep learning với ít parameter.\n",
        "- Ít parameter để học hơn nêu so với fully connected layer.\n",
        "\n",
        "**Disadvantages:**\n",
        "- Tương đối(Comparatively) complex to design and maintain\n",
        "- Comparatively slow (depends on the number of hidden layers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcbvabMZuwX1"
      },
      "source": [
        "###**Convolutional layer**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da_kkUDZvN9F"
      },
      "source": [
        "**Vấn đề của fully connected neural network với xử lý ảnh**\n",
        "\n",
        "Ảnh màu 64 * 64 được biểu diễn dưới dạng 1 tensor 64 * 64 * 3. Nên để biểu thị hết nội dung của bức ảnh thì cần truyền vào input layer tất cả các pixel (64 * 64 *3 = 12288). Nghĩa là input layer giờ có 12288 nodes.\n",
        "\n",
        "![Input an hidden layer1](https://i2.wp.com/nttuan8.com/wp-content/uploads/2019/03/input_hd1.png?w=552&ssl=1)\n",
        "\n",
        "Giả sử số lượng node trong hidden layer 1 là 1000. Số lượng weight W giữa input layer và hidden layer 1 là 12288 * 1000 = 12288000, số lượng bias là 1000 => tổng số parameter là: 12289000. Đấy mới chỉ là số parameter giữa input layer và hidden layer 1, trong model còn nhiều layer nữa, và nếu kích thước ảnh tăng, ví dụ 512 * 512 thì số lượng parameter tăng cực kì nhanh => Cần giải pháp tốt hơn !!!\n",
        "\n",
        "Nhận xét:\n",
        "- Trong ảnh các pixel ở cạnh nhau thường có liên kết với nhau hơn là những pixel ở xa.\n",
        "-Trong phép tính convolution trong ảnh, chỉ 1 kernel được dùng trên toàn bộ bức ảnh. Hay nói cách khác là các pixel ảnh **chia sẻ** hệ số với nhau.\n",
        "\n",
        "\n",
        "**=> Áp dụng phép tính convolution vào layer trong neural network ta có thể giải quyết được vấn đề lượng lớn parameter mà vẫn lấy ra được các đặc trưng của ảnh.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWxsbT8HvZAR"
      },
      "source": [
        "**Convolutional layer đầu tiên**\n",
        "\n",
        "Phép tính convolution thực hiện trên ảnh xám với biểu diễn ảnh dạng ma trận\n",
        "\n",
        "![convolution in grayimage](https://i1.wp.com/nttuan8.com/wp-content/uploads/2019/03/giphy.gif?resize=474%2C345&ssl=1)\n",
        "\n",
        "Tuy nhiên ảnh màu có tới 3 channels red, green, blue nên khi biểu diễn ảnh dưới dạng tensor 3 chiều. Nên ta cũng sẽ định nghĩa kernel là 1 tensor 3 chiều kích thước k * k * 3.\n",
        "\n",
        "![Phep tinh convolution tren anh mau voi k=3](https://i0.wp.com/nttuan8.com/wp-content/uploads/2019/03/kernel.png?w=326&ssl=1)\n",
        "\n",
        "![Bieu dien](https://i0.wp.com/nttuan8.com/wp-content/uploads/2019/03/conv.png?resize=768%2C515&ssl=1)\n",
        "\n",
        "\n",
        "![convolution](https://i2.wp.com/nttuan8.com/wp-content/uploads/2019/03/conv.gif?resize=1000%2C562&ssl=1)\n",
        "\n",
        "\n",
        "Nhận xét:\n",
        "- Output Y của phép tính convolution trên ảnh màu là 1 matrix.\n",
        "- Có 1 **hệ số bias** được cộng vào sau bước tính tổng các phần tử của phép tính element-wise\n",
        "\n",
        "**Padding and Stride**\n",
        "\n",
        "Với mỗi kernel khác nhau ta sẽ học được những đặc trưng khác nhau của ảnh, nên trong mỗi convolutional layer ta sẽ dùng nhiều kernel để học được nhiều thuộc tính của ảnh. Vì mỗi kernel cho ra output là 1 matrix nên k kernel sẽ cho ra k output matrix. Ta kết hợp k output matrix này lại thành 1 tensor 3 chiều có chiều sâu k.\n",
        "\n",
        "\n",
        "![Convolutional layer dau tien](https://i0.wp.com/nttuan8.com/wp-content/uploads/2019/03/conv1.png?w=855&ssl=1)\n",
        "\n",
        "Convolutional layer đầu tiên\n",
        "\n",
        "Output của convolutional layer đầu tiên sẽ thành input của convolutional layer tiếp theo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVomeikVvjv2"
      },
      "source": [
        "**Convolutional layer tổng quát**\n",
        "\n",
        "Giả sử input của 1 convolutional layer tổng quát là tensor kích thước H * W * D.\n",
        "\n",
        "Kernel có kích thước F * F * D (kernel luôn có depth bằng depth của input và F là số lẻ), stride: S, padding: P.\n",
        "\n",
        "Convolutional layer áp dụng K kernel.\n",
        "=> Output của layer là tensor 3 chiều có kích thước: $\\displaystyle (\\frac{H-F+2P}{S} + 1) * (\\frac{W-F+2P}{S} + 1) * K$\n",
        "\n",
        "![Convolution layer](https://i2.wp.com/nttuan8.com/wp-content/uploads/2019/03/conv2-1.png?w=991&ssl=1)\n",
        "\n",
        "Lưu ý:\n",
        "- Output của convolutional layer sẽ qua hàm activation function trước khi trở thành input của convolutional layer tiếp theo.\n",
        "-Tổng số parameter của layer: Mỗi kernel có kích thước F * F * D và có 1 hệ số bias, nên tổng parameter của 1 kernel là F * F * D + 1. Mà convolutional layer áp dụng K kernel => Tổng số parameter trong layer này là K * (F * F * D + 1). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxxu2stlvpcV"
      },
      "source": [
        "###**Pooling layer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "craK-7AEBVeN"
      },
      "source": [
        "\n",
        "Pooling layer thường được dùng giữa các convolutional layer, để giảm kích thước dữ liệu nhưng vẫn giữ được các thuộc tính quan trọng. Kích thước dữ liệu giảm giúp giảm việc tính toán trong model.\n",
        "\n",
        "Gọi pooling size kích thước K * K. Input của pooling layer có kích thước H * W *  D, ta tách ra làm D ma trận kích thước H * W. Với mỗi ma trận, trên vùng kích thước K * K trên ma trận ta tìm maximum hoặc average của dữ liệu rồi viết vào ma trận kết quả. Quy tắc về stride và padding áp dụng như phép tính convolution trên ảnh.\n",
        "\n",
        "![Pooling layer](https://i1.wp.com/nttuan8.com/wp-content/uploads/2019/03/pooling.gif?resize=396%2C248&ssl=1)\n",
        "\n",
        "max pooling layer với size=(3,3), stride=1, padding=0\n",
        "\n",
        "Nhưng **hầu hết** khi dùng pooling layer thì sẽ dùng size=(2,2), stride=2, padding=0. Khi đó output width và height của dữ liệu giảm đi một nửa, depth thì được giữ nguyên .\n",
        "\n",
        "![Sau pooling layer](https://i0.wp.com/nttuan8.com/wp-content/uploads/2019/03/pooling.jpeg?w=514&ssl=1)\n",
        "\n",
        "Có 2 loại pooling layer phổ biến là: max pooling và average pooling.\n",
        "\n",
        "![Vi du ve poolling layer](https://i1.wp.com/nttuan8.com/wp-content/uploads/2019/03/pooling_2.jpg?w=596&ssl=1)\n",
        "\n",
        "Ví dụ Max pooling layer:\n",
        "\n",
        "![Vi du max poolling layer](https://www.researchgate.net/profile/Wouter-Verschoof-Van-Der-Vaart/publication/331874666/figure/fig2/AS:738215627612160@1553015729189/Schematic-representation-of-a-convolution-and-pooling-layer-in-a-CNN.png)\n",
        "\n",
        "**Trong một số model người ta dùng convolutional layer với stride > 1 để giảm kích thước dữ liệu thay cho pooling layer.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGIyG8PKvxbe"
      },
      "source": [
        "###**Fully connected layer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sb-S62lIBZT2"
      },
      "source": [
        "\n",
        "Sau khi ảnh được truyền qua nhiều convolutional layer và pooling layer thì model đã học được tương đối các đặc điểm của ảnh (ví dụ mắt, mũi, khung mặt,…) thì tensor của output của layer cuối cùng, kích thước H * W * D, sẽ được chuyển về 1 vector kích thước (H * W * D)\n",
        "\n",
        "![fully connected layer](https://i1.wp.com/nttuan8.com/wp-content/uploads/2019/03/flattern.png?resize=768%2C367&ssl=1)\n",
        "\n",
        "Sau đó ta dùng các fully connected layer để kết hợp các đặc điểm của ảnh để ra được output của model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brd27XuhwDKy"
      },
      "source": [
        "###**Visualise convolutional neural network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L17wcSgBcIb"
      },
      "source": [
        "Mô hình convolutional neural network:\n",
        "\n",
        "Input image -> Convolutional layer (Conv) + Pooling layer (Pool) -> Fully connected layer (FC) -> Output.\n",
        "\n",
        "![mo hinh](https://i0.wp.com/nttuan8.com/wp-content/uploads/2019/03/cnn.png?w=1024&ssl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAfzxzatSNkZ"
      },
      "source": [
        "##Radial Basis Function Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI-rxWVJSSWF"
      },
      "source": [
        "##Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqCZJKyQSXQS"
      },
      "source": [
        "##Sequence to sequence models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIgqGYRMSYnp"
      },
      "source": [
        "##Modular Neural Network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYFR7koue6yX"
      },
      "source": [
        "#5. Examples of deep learning applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXfxK84Me_ul"
      },
      "source": [
        "#6. Limitations of deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ziTsKd50Fzc"
      },
      "source": [
        "**Data labeling**\n",
        "\n",
        "Hầu hết các mô hình AI hiện tại đều được đào tạo thông qua \"học có giám sát\". Nó có nghĩa là con người phải gắn nhãn và phân loại dữ liệu cơ bản, đây có thể là một công việc khá lớn và dễ xảy ra lỗi. Ví dụ: các công ty phát triển công nghệ xe hơi tự lái đang thuê hàng trăm người chú thích thủ công hàng giờ các dữ liệu video từ các phương tiện lấy mẫu để giúp đào tạo các hệ thống này."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cB1F6Gc0Nqm"
      },
      "source": [
        "**Obtain huge training datasets**\n",
        "\n",
        "Đòi hỏi các tập dữ liệu đào tạo không chỉ được gắn nhãn mà còn phải đủ rộng và phổ quát.\n",
        "\n",
        "Phương pháp deep learning đòi hỏi hàng nghìn lượt quan sát để các mô hình trở nên tương đối tốt trong các nhiệm vụ phân loại và, trong một số trường hợp, hàng triệu để chúng thực hiện ở cấp độ con người. Không ngạc nhiên khi học sâu rất nổi tiếng trong các công ty công nghệ khổng lồ; họ đang sử dụng dữ liệu lớn để tích lũy hàng petabyte dữ liệu. Nó cho phép họ tạo ra một mô hình học sâu ấn tượng và có độ chính xác cao.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qDocXWy0SKT"
      },
      "source": [
        "**Explain a problem**\n",
        "\n",
        "Các mô hình lớn và phức tạp có thể khó giải thích theo nghĩa của con người. Đó là một lý do khiến việc chấp nhận một số công cụ AI chậm, Khả năng diễn đạt thực sự cần thiết.\n",
        "\n",
        "Hơn nữa, khi ứng dụng AI ngày càng mở rộng, các yêu cầu quy định cũng có thể thúc đẩy nhu cầu về \"các mô hình AI dễ giải thích\" hơn là một \"mô hình AI không thể giải thích được\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLGnWTBIfFL4"
      },
      "source": [
        "#7. Some Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTTJ5bGeYPwa"
      },
      "source": [
        "##Full Connected Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkySrv6ruo9o"
      },
      "source": [
        "\n",
        "![Mo hinh neural network](https://i2.wp.com/nttuan8.com/wp-content/uploads/2019/03/nn-2.png?resize=768%2C631&ssl=1)\n",
        "\n",
        "Mô hình hình Neural Network\n",
        "\n",
        "\n",
        "Mỗi hidden layer được gọi là **fully connected layer**, tên gọi theo đúng ý nghĩa, mỗi node trong hidden layer được kết nối với tất cả các node trong layer trước. Cả mô hình được gọi là **fully connected neural network (FCN).**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9J_8k6LfK93"
      },
      "source": [
        "##Tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAzWle4hn15F"
      },
      "source": [
        "\n",
        "Khi dữ liệu biểu diễn dạng 1 chiều, người ta gọi là vector, mặc định khi viết vector sẽ viết dưới dạng cột.\n",
        "\n",
        "Khi dữ liệu dạng 2 chiều, người ta gọi là ma trận, kích thước là số hàng * số cột. \n",
        "\n",
        "![Vector v kich thuoc n, ma tran W kich thuoc m*n](https://i1.wp.com/nttuan8.com/wp-content/uploads/2019/03/CodeCogsEqn-8-1.gif?resize=320%2C89&ssl=1)\n",
        "\n",
        "Vector v kích thước n, ma trận W kích thước m*n\n",
        "\n",
        "**Khi dữ liệu nhiều hơn 2 nhiều thì sẽ được gọi là _Tensor_, ví dụ như dữ liệu có 3 chiều.**\n",
        "\n",
        "_Ví dụ biểu diễn ảnh màu kích thước 28*28, biểu diễn dưới dạng tensor 28*28*3_\n",
        "\n",
        "![Vi du bieu dien tensor](https://i2.wp.com/nttuan8.com/wp-content/uploads/2019/03/tensor.jpg?w=638&ssl=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja-JFmQomDVd"
      },
      "source": [
        "##Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVDJwIaWmNpy"
      },
      "source": [
        "###Multiplication matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElgZYuNcnAUN"
      },
      "source": [
        "Phép tính nhân ma trận A * B chỉ thực hiện được khi số cột của A bằng số hàng của B, hay A có kích thước m * n và B có kích thước n * k.\n",
        "\n",
        "Ma trận C = A * B thì C có kích thước m * k và $C[i,j]=∑A[i,k]∗B[k,j]$\n",
        "\n",
        "\n",
        "![multi matrix](https://i0.wp.com/nttuan8.com/wp-content/uploads/2019/02/CodeCogsEqn-1-1.gif?resize=264%2C45&ssl=1)\n",
        "\n",
        "$C[1][1] = A[1][1] * B[1][1] + A[1][2] * B[2][1] = ax + cy$\n",
        "\n",
        "$C[2][1] = A[2][1] * B[1][1] + A[2][2] * B[2][1] = bx + dy$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXKi01n3myNk"
      },
      "source": [
        "###Element-wise multiplication matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JswSj6Qjmk4Q"
      },
      "source": [
        "Ma trận A và B cùng kích thước m*n thì phép tính này cho ra ma trận C cùng kích thước m*n và C[i,j] = A[i,j] * B[i,j]. Hay là mỗi phần tử ở ma trận C bằng tích 2 phần tử tương ứng ở A và B.\n",
        "\n",
        "Kí hiệu C = A $\\otimes$ B, ví dụ: \n",
        "\n",
        "![Element-wise](https://i0.wp.com/nttuan8.com/wp-content/uploads/2019/02/CodeCogsEqn-4.gif?resize=400%2C45&ssl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RzVy0zoj0NX"
      },
      "source": [
        "##Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zPNunT9j56B"
      },
      "source": [
        "###Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OX-bRBtkemo"
      },
      "source": [
        "Ví dụ thực hiện trên ảnh xám, tức là ảnh được biểu diễn dưới dạng ma trận A kích thước m * n.\n",
        "\n",
        "**kernel** là một ma trận vuông kích thước k * k trong đó k là số lẻ. k có thể bằng 1, 3, 5, 7, 9,… Ví dụ kernel kích thước 3 * 3\n",
        "![kernel](https://i1.wp.com/nttuan8.com/wp-content/uploads/2019/03/CodeCogsEqn-13-1.gif?resize=135%2C67&ssl=1)\n",
        "\n",
        "Kí hiệu phép tính convolution ($\\otimes$), kí hiệu $Y=X⊗W$\n",
        "\n",
        "Với mỗi phần tử $x_{ij}$​ trong ma trận $X$ lấy ra một ma trận có kích thước bằng kích thước của kernel $W$ có phần tử $x_{ij}$​ làm trung tâm (đây là vì sao kích thước của kernel thường lẻ) gọi là ma trận $A$. Sau đó tính tổng các phần tử của phép tính element-wise của ma trận $A$ và ma trận $W$, rồi viết vào ma trận kết quả $Y$.\n",
        "\n",
        "![convolution example](https://i0.wp.com/nttuan8.com/wp-content/uploads/2019/03/c1.png?resize=768%2C356&ssl=1)\n",
        "\n",
        "Ví dụ khi tính tại $x_{22}$ (ô khoanh đỏ trong hình), ma trận $A$ cùng kích thước với $W$, có $x_{22}$​ làm trung tâm có màu nền da cam như trong hình. Sau đó tính $y_{11} = sum(A \\otimes W) = x_{11}*w_{11} + x_{12}*w_{12} + x_{13}*w_{13} + x_{21}*w_{21} + x_{22}*w_{22} + x_{23}*w_{23} + x_{31}*w_{31} + x_{32}*w_{32} + x_{33}*w_{33} = 4$. Và làm tương tự với các phần tử còn lại trong ma trận.\n",
        "\n",
        "Để ý thấy ma trận Y có kích thước nhỏ hơn ma trận $X$. Kích thước của ma trận $Y$ là $(m-k+1) * (n-k+1)$.\n",
        "\n",
        "![convoled Feature](https://i1.wp.com/nttuan8.com/wp-content/uploads/2019/03/giphy.gif?resize=526%2C384&ssl=1)\n",
        "\n",
        "Do với cách tính như vậy thì sẽ không thể nào tính được các điểm ngoài viền. Ví dụ như $x_{11}$\n",
        "\n",
        "![Matrix border](https://i0.wp.com/nttuan8.com/wp-content/uploads/2019/03/c2.png?resize=300%2C269&ssl=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RIIGJpuj-Rx"
      },
      "source": [
        "###Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT8L7n2zqv5R"
      },
      "source": [
        "Như ở trên thì mỗi lần thực hiện phép tính convolution xong thì kích thước ma trận $Y$ đều nhỏ hơn $X$. Tuy nhiên giờ ta muốn ma trận $Y$ thu được có kích thước bằng ma trận $X =>$ Tìm cách giải quyết cho các phần tử ở viền $=>$ Thêm giá trị 0 ở viền ngoài ma trận $X$.\n",
        "\n",
        "![Padding](https://i0.wp.com/nttuan8.com/wp-content/uploads/2019/03/c3.png?w=490&ssl=1)\n",
        "\n",
        "Phép tính này gọi là convolution với **padding=1.** Padding=k nghĩa là thêm k vector 0 vào mỗi phía của ma trận."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYVDFMSekEDr"
      },
      "source": [
        "###Stride"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orDjYm5XrkyQ"
      },
      "source": [
        "Như ở trên ta thực hiện tuần tự các phần tử trong ma trận $X$, thu được ma trận $Y$ cùng kích thước ma trận $X$, ta gọi là **stride=1**.\n",
        "\n",
        "![stride=1,padding=1](https://i0.wp.com/nttuan8.com/wp-content/uploads/2019/03/c4.png?w=491&ssl=1)\n",
        "\n",
        "stride=1,padding=1\n",
        "\n",
        "Tuy nhiên nếu stride=k (k > 1) thì ta chỉ thực hiện phép tính convolution trên các phần tử $x_{1+i*k,1+j*k}$​. Ví dụ k = 2.\n",
        "\n",
        "![stride=2](https://i2.wp.com/nttuan8.com/wp-content/uploads/2019/03/c5.png?w=492&ssl=1)\n",
        "\n",
        "\n",
        "**Hiểu đơn giản là bắt đầu từ vị trí $x_{11}$​ sau đó nhảy k bước theo chiều dọc và ngang cho đến hết ma trận $X$.**\n",
        "\n",
        "\n",
        "Công thức tổng quát cho phép tính convolution của ma trận $X$ kích thước m * n với kernel kích thước k * k, stride = s, padding = p ra ma trận $Y$ kích thước $\\displaystyle(\\frac{m-k+2p}{s}+1) * (\\frac{n-k+2p}{s}+1)$\n",
        "\n",
        "Stride thường dùng để giảm kích thước của ma trận sau phép tính convolution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3zVmvZ6kOXw"
      },
      "source": [
        "###Meaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rCATLtvtJ1x"
      },
      "source": [
        "Mục đích của phép tính convolution trên ảnh là làm mở, làm nét ảnh; xác định các đường;… Mỗi kernel khác nhau thì sẽ phép tính convolution sẽ có ý nghĩa khác nhau. Ví dụ:\n",
        "\n",
        "![Example](https://i2.wp.com/nttuan8.com/wp-content/uploads/2019/03/purpose.png?w=670&ssl=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OHFlVd7-OHV"
      },
      "source": [
        "##Parameters vs Hyperparameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mIB3G7j-Z0i"
      },
      "source": [
        "Sự khác biệt chính giữa **Parameters** vs **Hyperparameters** là các **Parameters** được mô hình học trong thời gian đào tạo, trong khi **Hyperparameters** có thể được thay đổi trước khi đào tạo mô hình và ta phải tự chọn qua kinh ngiệm.\n",
        "\n",
        "Các **Parameters** của mạng neuron sâu là $W$ và $b$, mô hình cập nhật trong bước **Backward propagation**. Mặt khác, có rất nhiều **Hyperparameters** cho một DNN,, bao gồm:\n",
        "- Learning rate $\\alpha$\n",
        "- Number of iterations\n",
        "- Number of hidden layers\n",
        "- Units in each hidden layer\n",
        "- Chọn activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEHQpw7HFbf6"
      },
      "source": [
        "##Chain rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A29_ADeNFh-n"
      },
      "source": [
        "đạo hàm của hàm hợp"
      ]
    }
  ]
}