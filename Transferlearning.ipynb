{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transferlearning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9jJ7CeZDqVf"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "**Transfer learning** là việc ứng dụng kỹ năng/tri thức học được từ vấn đề này với ứng dụng này sang vấn đề khác với ứng dụng khác.\n",
        "\n",
        "![image](https://forum.machinelearningcoban.com/uploads/default/optimized/2X/8/852acb7de078b7423441daae61a4c209aeba86d7_2_690x370.png)\n",
        "\n",
        "Trong machine learning và deep learning, thì transfer learning là kỹ thuật cho phép tận dụng những gì mình học được từ tập dữ liệu/ ứng dụng/ kiến trúc này sang tập dữ liệu/ứng dụng/kiến trúc khác. Khác với học máy thông thường, từng nhiệm vụ sẽ có một hệ thống học riêng như ở hình ví dụ trên.\n",
        "\n",
        "**Lưu ý:** Kỹ thuật Transfer Learning của chung cho ML chứ không phải của riêng của DL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnMAJf8lHjZ_"
      },
      "source": [
        "**Phân loại Transfer Learning:**\n",
        "1.   Phân loại theo chủ đề nghiên cứu:\n",
        "  *   Transfer cái gì: phần nào của tri thức đã học có thể transfer? Vì một số tri thức có tính đặc thù riêng của nhiệm vụ, và có các tri thức chung giữa các nhiệm vụ khác nhau. Về cơ bản, tri thức chung là có thể transfer được.\n",
        "  *   Transfer khi nào: trường hợp nào (không) nên transfer. Transfer learning có thể sẽ không đem lại lợi ích nếu hai nhiệm vụ không có mối sự liên hệ, tương quan (còn được biết đến là negative transfer)\n",
        "  *   Transfer như thế nào: Cách thức thực hiện transfer như thế nào cho hiệu quả.\n",
        "\n",
        "2.   Phân loại theo Domains:\n",
        "  *   Inductive transfer learning: khác biệt về nhiệm vụ (task)\n",
        "      *   Cùng domain: Transfer learning để có được chất lượng tốt cho ứng dụng khác. Ví dụ transfer learning từ việc nhận dạng giới tính sang nhận dạng tuổi. Khác với multi-task learning, là thiết kế thuật toán tốt cho cả nhận dạng giới tính và tuổi, transfer learning ở đây chỉ muốn nhận dạng tuổi tốt hơn thôi.\n",
        "      *   Khác domain: Dữ liệu từ source domain (miền nguồn) không có nhãn.\n",
        "  *   Transductive transfer learning: giống nhau về nhiệm vụ nhưng khác biệt về domain\n",
        "      *   Bài toán Segmentation (Sự phân khúc) cho ảnh y tế và ảnh thông thường.\n",
        "  *   Unsupervised transfer learning. Giống inductive transfer learning nhưng target task ở dạng un-supervised learning điển hình như clustering (nhóm), giảm chiều dữ liệu (dimension reduction), .v.v.\n",
        "\n",
        "3.   Phân loại theo hướng tiếp cận:\n",
        "  *   Instance transfer: Dùng lại tri thức từ source domain từ một số trường hợp/ví dụ chính.\n",
        "  *   Feature representation transfer: Giảm thiểu sự khác biệt giữa 2 domains.\n",
        "  *   Parameter transfer: Dựa trên giả thiết các task liên hệ với nhau có cùng phân bố về bộ thông số hyper parameters.\n",
        "  *   Relational - Knowledge transfer: Hướng tới giải quyết vấn đề khi dữ liệu không độc lập và phân bố ngẫu nhiên, thường thấy ở ứng dụng liên quan tới mạng xã hội.\n",
        "  **Liên hệ giữa phân loại theo hướng tiếp cận và domain như sau:**\n",
        "  ![image](https://forum.machinelearningcoban.com/uploads/default/optimized/2X/e/e69b46e61d9f5135844a7ebf9df350a1eb2a1308_2_690x88.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahuy-3zZJek5"
      },
      "source": [
        "# Transfer Learning trong Deep Learning\n",
        "\n",
        "**Tại sao nên dùng Transfer Learning trong Deep Learning?**\n",
        "*   **Không đủ dữ liệu:** Deep Learning cần rất nhiều dữ liệu, và ngốn rất nhiều tài nguyên để học trên tập dữ liệu và ứng dụng đó và việc huấn luyện Deep Learning trên tập dữ liệu ít phần nhiều là không hiệu quả. Vậy ngoài kỹ thuật data augmentation trước đó, transfer learning cũng là một giải pháp cho vấn đề này.\n",
        "    *   Sơ lược về Data Augmentation (Phép xử lý tăng số lượng dữ liệu dataset):\n",
        "        *   **Data Augmentation**: Là kỹ thuật đơn giản nhất bằng việc xử lý đơn giản dữ liệu sẵn có bằng các phép tuyến tính hay phi tuyến.\n",
        "        ![image](https://forum.machinelearningcoban.com/uploads/default/optimized/2X/b/bd65a6de81cb00cba88e94e5822fee7b84220374_2_690x280.jpeg)\n",
        "        ![image](https://forum.machinelearningcoban.com/uploads/default/optimized/2X/e/e4ed0b545fb8d6e018e6fbac8805036fe9d003eb_2_690x281.jpeg)\n",
        "        ![image](https://forum.machinelearningcoban.com/uploads/default/optimized/2X/9/98439a847418d4cd838b6b747a04fd5a2ba37e73_2_690x388.jpeg)\n",
        "          *   Original (Ảnh gốc).\n",
        "          *   Flip (Lật): lật theo chiều dọc, ngang miễn sao ý nghĩa của ảnh (label) được giữ nguyên hoặc suy ra được. Ví dụ nhận dạng quả bóng tròn, thì mình lật kiểu gì cũng ra quả bóng. Còn với nhận dạng chữ viết tay, lật số 8 vẫn là 8, nhưng 6 sẽ thành 9 (theo chiều ngang) và không ra số gì theo chiều dọc. Còn nhận dạng ảnh y tế thì việc bị lật trên xuống dưới là không bao giờ sảy ra ở ảnh thực tế --> không nên lật làm gì.\n",
        "          *   Random crop (Cắt ngẫu nhiên): cắt ngẫu nhiên một phần của bức ảnh. Lưu ý là khi cắt phải giữ thành phần chính của bức ảnh mà ta quan tâm. Như ở nhận diện vật thể, nếu ảnh được cắt không có vật thể, vậy giá trị nhãn là không chính xác.\n",
        "          *   Color shift (Chuyển đổi màu): Chuyển đổi màu của bức ảnh bằng cách thêm giá trị vào 3 kênh màu RGB. Việc này liên quan tới ảnh chụp đôi khi bị nhiễu --> màu bị ảnh hưởng.\n",
        "          *   Noise addition (Thêm nhiễu): Thêm nhiễu vào bức ảnh. Nhiễu thì có nhiều loại như nhiễu ngẫu nhiên, nhiễu có mẫu, nhiễu cộng, nhiễu nhân, nhiễu do nén ảnh, nhiễu mờ do chụp không lấy nét, nhiễu mờ do chuyển động…\n",
        "          *   Information loss (Mất thông tin): Một phần của bức hình bị mất. Có thể minh họa trường hợp bị che khuất.\n",
        "          *   Constrast change (Đổi độ tương phản): thay độ tương phản của bức hình, độ bão hòa.\n",
        "          *   Geometry based: Đủ các thể loại xoay, lật, scale, padding, bóp hình, biến dạng hình.\n",
        "          *   Color based: giống như trên, chi tiết hơn chia làm (i) tăng độ sắc nét, (ii) tăng độ sáng, (iii) tăng độ tương phản hay (iv) đổi sang ảnh negative - âm bản.\n",
        "          *   Noise/occlusion: Chi tiết hơn các loại nhiễu.\n",
        "          *   Whether: thêm tác dụng cảu thời tiết như mưa, tuyết, sương mờ, …\n",
        "\n",
        "*   **Không đủ tài nguyên:** Việc học trên tập dữ liệu lớn rất ngốn nhiều tài nguyên. Transfer learning sẽ góp phần giảm phần nào thời lượng training.\n",
        "*   **Cải thiện chất lượng:** Rất nhiều trường hợp transfer learning cải thiện chất lượng dự đoán của Target Task so với việc train lại từ đầu. Lý do có thể do Source Network được train với dữ liệu lớn và học được tính khái quát hóa tốt hơn, hay việc train với Target Task trong khi mạng vẫn có thông tin của Source Task cho tách động của multi-task learning.\n",
        "![image](https://forum.machinelearningcoban.com/uploads/default/optimized/2X/7/7e406a1326e6b34fb89c1087bb4dfd1c4733f38b_2_690x340.png)\n",
        "\n",
        "*Như trong bài talk của Andrew Ng tại NIPS 2016[5], transfer learing sẽ là cầu nối về chất lượng của machine learning giữa un-supervised learning và supervised learning. Transfer learning giúp giải quyết các vấn đề mà không có dữ liệu, các vấn đề mới chưa được học trước đó.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHezitX63aFi"
      },
      "source": [
        "**Phương thức Transfer Learning**\n",
        "1.   Transfer Learning as a starting point:\n",
        "\n",
        "    Dùng kết quả của mạng pre-trained như VGG19, InceptionNet, ResNet101 làm kết quả dự đoán ban đầu. Thiết kế thuật toán fusion kết quả của nhiều mạng pre-trained.\n",
        "    ![image](https://forum.machinelearningcoban.com/uploads/default/optimized/2X/2/200736d6adeada04fedd5cc675781d1042160137_2_690x268.png)\n",
        "2.   Transfer learning for representation:\n",
        "\n",
        "    Lấy đầu ra của layer gần cuối và coi như là feature vector. Trong bài toán nhận dạng vật thể, mạng neural network có thể coi như một thuật toán tự học cách biểu diễn dữ liệu cho bài toán nhận dạng (learned representation) với phần nhận dạng đơn giản (thường dùng Softmax và one-hot coding). Vector biểu diễn đó có thể đưa vô các thuật toán phân loại phức tạp hơn như SVM.\n",
        "    ![image](https://forum.machinelearningcoban.com/uploads/default/optimized/2X/0/043338f78e05aea87c5251f80f70e762a07b8a82_2_690x314.png)\n",
        "3.   Warm restart via fine tuning:\n",
        "\n",
        "    Sử dụng mạng đã train từ tập dữ liệu lớn như ImageNet, rồi train lại với dữ liệu khác ở learning rate nhỏ hơn.\n",
        "    ![image](https://forum.machinelearningcoban.com/uploads/default/optimized/2X/b/b2d950a8de05c75d558be0742d0683ae24b40cf7_2_690x316.png)\n",
        "    Thông thường sẽ không học các giá trị weight ở các layer đầu (do mấy layer đầu thường là biểu diễn dữ liệu ở dạng low-level vision - thị giác máy cấp thấp, chứa thông tin cơ bản của ảnh như cạnh, texture) còn layer cuối thường chứa thông tin ở cấp cao, liên quan tới ứng dụng, task cụ thể. Tuy nhiên tùy mối tương quan của hai task (source và target task) và thiết lập learning rate tương ứng.\n",
        "    ![image](https://forum.machinelearningcoban.com/uploads/default/optimized/2X/7/7ded0c5b8fcb22fbf149291baf0d86da6f470748_2_690x345.png)\n",
        "4.   Pre-trained Model:\n",
        "\n",
        "    Việc sử dụng các pre-train model không chỉ dừng lại ở việc freezinng và fine-tuning các layer như trên. Mà có thể ghép nối nhiều network lại với nhau và freeze, fine-tune tương ứng. Việc sử dụng pre-trained model góp phần làm giảm đáng kể thời gian training và tăng chất lượng dự đoán.\n",
        "\n",
        "    Ví dụ muốn thực hiện xóa nhiễu (image denoising) và tăng độ phân giải (image super-resolution), thì có thể tận dụng hai network DenoiseNet và SRNet. Ví dụ input của SRNet là output của DenoiseNet và tiến hành train tuần tự hoặc train cùng lúc cả hai mạng. Giống như kiểu Network trong network, hay train mạng tạo sinh GAN ấy.\n",
        "\n",
        "    Hoặc có thể phối hợp giữa warm re-start và pre-trained model như, Net1 dùng VGG19 fine-tune 2 layer cuối cùng. Net2 dùng VGG19 làm warm start, và train cả 2 thằng cùng lúc. Với việc fusion (sớm hoặc muộn) features của 2 network và chọn làm features cho việc dựa đoán.\n",
        "\n",
        "    Nếu bạn làm về GAN, khôi phục ảnh, super-resolution sẽ biết tới việc Pre-trained VGG net được sử dụng làm perceptual loss - một hàm loss rất nổi tiếng. Thay vì tính hàm loss theo mean square error (MSE - hay L2 norm), structure similarity (SSIM) của dữ liệu gốc và dữ liệu dự đoán thì perceptual loss này so sánh sự khác biệt về MSE giứa các intermediate feature của mạng VGG(gốc) và VGG(dự đoán).\n",
        "5.   Transfer learnign via Multi-task learning:\n",
        "\n",
        "    Để train một mạng NetA có khả năng transfer tốt hơn, thay vì ta train NetA với một task, thì sẽ train nó với nhiều task. Điều này dẫn tới phần lớn các layer trừ một vài layer cuối sẽ có đặc tính khái quát hóa tốt hơn từ đó dẫn đến transfer learning tốt hơn cho các ứng dụng, dữ liệu khác.\n",
        "    ![image](https://forum.machinelearningcoban.com/uploads/default/original/2X/1/126039e0011743f227de89a36c7cadad3a8075bb.png)\n",
        "6.   Transfer learning via generalization: <...>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jeon4SMyI5f"
      },
      "source": [
        "# Ứng dụng Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eA5xci1yUR0"
      },
      "source": [
        "**Domain Adaptation** <...>\n",
        "\n",
        "Là một mảng trong **transfer learning** khi phân bố (probability distribution) của dữ liệu gốc (source domain) khác (nhưng có liên hệ) với miền dữ liệu cần transfer (target domain). Ví dụ điển hình của **domain adaptation** là ứng dụng bộ lọc spam (Spam filtering problem). Mô hình lọc spam được học từ user này (source distribution) sang một user khác có sự khác biệt đáng kể về email nhận. **Domain adaptation** cũng được dùng để học các nguồn dữ liệu không có mối liên hệ trực tiếp. Ngoài ra, **domain adaptation** với nghiên nguồn dữ liệu khác nhau còn gọi là **multiple-source domain adaptation**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw9sQXKl2Vrr"
      },
      "source": [
        "**Domain Confusion** <...>\n",
        "\n",
        "Vì intermediate feature (đặc trưng trung gian) ở mỗi layer mang các thông tin khác nhau (output của layer đầu thường mang thông tin cạnh, texture, trong khi output của layer cuối mang thông tin tổng quát hóa về bài toán .v.v.) dẫn đến ta hoàn toàn có thể tận dụng features ở các layer khác nhau (Không nhất thiết phải là ở layer gần cuối).\n",
        "\n",
        "Việc tận dụng các intermediate feature này có thể giúp học các features có thể dùng cho nhiều domain khác nhau (domain-invariant). Ý tưởng cơ bản là thêm các hàm los để mô hình gốc học các đặc tính chung giữa các domain khác nhau nhiều hơn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l324IXn34qvP"
      },
      "source": [
        "**‘X’-Shot Learning**\n",
        "\n",
        "**'X'-Shot Learning** bao gồm:\n",
        "![image](https://forum.machinelearningcoban.com/uploads/default/original/2X/e/e0e9b8604725029dd5930785ce7de6afc025fb4f.png)\n",
        "1.   One/Few-Shot Learning:\n",
        "\n",
        "    Ví dụ như một số class không thể có dữ liệu label, hoặc khi ta có một mẫu label mới, việc training lại từ đầu là rất lãng phí. Và đây cũng chính là động lực cho sự phát triển của các thuật toán “One”-Shot Learning, một nhánh của transfer learning. Chỉ cần một vài mẫu dữ liệu nhỏ, One-Short learning có thể học được tác vụ đó ở chất lượng tương đối tốt.\n",
        "\n",
        "    Ý tưởng chụ đạo của One shot learning đó là thay vì học cách phân loại cụ thể các lớp (100, hay 1000 lớp), mạng sẽ học cách phân biệt hai class khác nhau giống như là học một hàm khoảng cách (a distance function). Network sẽ học cách khái quát hóa việc ảnh thuộc cùng 1 class hay khác class trên tập dữ liệu huấn luyện, và từ đó ứng dụng cho tập dữ liệu chưa nhìn thấy trước đó.\n",
        "\n",
        "    Khác với việc huấn luyện Network thông thường, huấn luyện mạng One-shot learning gồm hai bước cơ bản:\n",
        "      *   Verification task: Nhận 2 dữ liệu đầu vào, phân loại nhị phân xem chúng thuộc cùng 1 class hay không.\n",
        "      *   Ứng dụng mạng verification để phân loại ảnh mới xem có thuộc cùng class hay không.\n",
        "        ![image](https://forum.machinelearningcoban.com/uploads/default/original/2X/9/9d6d2454de5c870155bde7652a54160e62ecaa3f.png)\n",
        "\n",
        "\n",
        "2.   Zero-shot learning:<...>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVb_vufW_kr9"
      },
      "source": [
        "# Fine Tuning\n",
        "\n",
        "Fine-tuning là bạn lấy 1 pre-trained model, tận dụng 1 phần hoặc toàn bộ các layer, thêm/sửa/xoá 1 vài layer/nhánh để tạo ra 1 model mới. Thường các layer đầu của model được freeze (đóng băng) lại - tức weight các layer này sẽ không bị thay đổi giá trị trong quá trình train. Lý do bởi các layer này đã có khả năng trích xuất thông tin mức trìu tượng thấp , khả năng này được học từ quá trình training trước đó. Ta freeze lại để tận dụng được khả năng này và giúp việc train diễn ra nhanh hơn (model chỉ phải update weight ở các layer cao). Có rất nhiều các Object detect model được xây dựng dựa trên các Classifier model.\n",
        "\n",
        "![image](https://images.viblo.asia/393992af-6c3c-4a54-9cd6-9a47d430afee.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu5D0BtTBwd9"
      },
      "source": [
        ""
      ]
    }
  ]
}